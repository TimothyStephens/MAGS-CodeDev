# Configuration

api_keys:
  openai: "sk-..."
  anthropic: "sk-ant-..."
  gemini: "AIza..."

models:
  # -------------------------------------------------------------------
  # PRIMARY CODER
  # -------------------------------------------------------------------
  # The heavy lifter. Use your strongest model with the largest token 
  # allowance here (e.g., gpt-4o, claude-3.5-sonnet, gemini-1.5-pro).
  coder:
    provider: "openai"
    model: "gpt-4o"

  # -------------------------------------------------------------------
  # SPECIALIZED AGENTS
  # -------------------------------------------------------------------
  tester:
    provider: "anthropic"
    model: "claude-3-5-sonnet-20240620"
  
  log_checker:
    provider: "google"
    model: "gemini-1.5-pro-latest"
    
  chat:
    provider: "openai"
    model: "gpt-4o"

  # -------------------------------------------------------------------
  # CODE REVIEWERS (Parallel Consensus)
  # -------------------------------------------------------------------
  # Add as many models as you want. The LangGraph will execute these 
  # concurrently and aggregate their feedback for the Coder agent.
  reviewers:
    - provider: "openai"
      model: "gpt-4o"
      
    - provider: "google"
      model: "gemini-1.5-pro-latest"
      
    - provider: "anthropic"
      model: "claude-3-5-sonnet-20240620"
      
    # Example of a Locally Hosted Model (e.g., Ollama, vLLM, LM Studio)
    # Uses the 'custom_openai' provider to route through LangChain's ChatOpenAI 
    # class but points to your local machine's port.
    - provider: "custom_openai" 
      model: "llama3:70b"          # The exact model name your local server expects
      base_url: "http://localhost:11434/v1" # Standard port for Ollama API
      api_key: "ollama"            # Dummy key to satisfy the Langchain client

settings:
  max_parallel_functions: 4             # How many functions to build concurrently
  docker_test_image: "mags-dev-env:latest" # Base image for isolated unit tests. Build with `docker build -t mags-dev-env:latest -f Dockerfile.dev .`
  timeout_per_function_mins: 15         # Max time before a LangGraph run aborts